{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac55ef4",
   "metadata": {},
   "source": [
    "# Train SVM classifiers for poet profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df53056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/menga/opt/anaconda3/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from format import *\n",
    "from features import *\n",
    "from svm_classifier import *\n",
    "import torch\n",
    "import torch.optim\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb0f7d",
   "metadata": {},
   "source": [
    "Load in the training data. Then extract features to turn poem into vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efa09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = get_text_to_gender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "394e4356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content    275\n",
       "gender     275\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does F vs M train split look like\n",
    "df_train.where(df_train['gender']=='F').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df24c2",
   "metadata": {},
   "source": [
    "Some features (BoW and prosodic attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9ea685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'y', 'la', 'el', 'que', 'en', 'a', 'su', 'del', 'al', 'tu', 'mi', 'con', 'un', 'los', 'se', 'las', 'no', 'por', 'es', 'me', 'más', 'sus', 'te', 'una', 'como', 'lo', 'si', 'amor', 'sin', 'Y', 'ni', 'vida', 'tus', 'le', 'alma', 'para', 'ya', 'mis', 'tan', 'cuando', 'o', 'luz', 'cielo', 'yo', 'pues', 'Dios', 'sol', 'ser', 'ha', 'entre', 'mundo', 'No', 'corazón', 'En', 'ojos', 'sobre', 'bien', 'La', 'El', 'qué', 'gloria', 'muerte', 'todo', 'tú', 'día', 'quien', 'cual', 'porque', 'ti', 'dulce', 'son', 'mar', 'De', 'donde', 'dolor', 'sólo', 'Si', 'noche', 'siempre', 'fue', 'tanto', 'triste', 'Mas', 'frente', 'fin', 'flores', 'voz', 'tiempo', 'tierra', 'mano', 'pecho', 'este', 'Oh', 'mas', 'hay', 'está', 'oh', 'tal', 'así', 'nos', 'oro', 'mí', 'hoy', 'Por', 'mal', 'hasta', 'mía', 'Qué', 'esta', 'vez', 'él', 'viento', 'Yo', 'dos', 'ella', 'e', 'aunque', 'he', 'fuego', 'llanto', 'llama', 'bajo', 'flor', 'aquel', 'pero', 'A', 'Es', 'Que', 'hombre', 'vano', 'gran', 'suerte', 'sombra', 'Ya', 'nombre', 'ver', 'fe', 'mil', 'ardiente', 'pura', 'suelo', 'otro', 'hermosa', 'nunca', 'Tú', 'mismo', 'sangre', 'divino', 'rosa', 'azul', 'aun', 'eres', 'canto', 'esperanza', 'nada', 'Cuando', 'feliz', 'cuanto', 'tiene', 'otra', 'memoria', 'paz', 'va', 'da', 'os', 'toda', 'todos', 'Al', 'también', 'pena', 'inmortal', 'aquí', 'ese', 'mujer', 'hermosura', 'Con', 'soy', 'fuerte', 'mientras', 'Pero', 'pensamiento', 'ante', 'tarde', 'quiero', 'morir', 'era', 'bella', 'tras', 'mío', 'Señor', 'vos', 'virtud', 'cuerpo', 'vuelo', 'ay', 'desde', 'olvido', 'fama', 'lágrimas', 'sueño', 'muy', 'manos', 'esa', 'amores', 'mayor', 'hace', 'boca', 'camino', 'veces', 'España', 'nuevo', 'Quién', 'luna', 'Amor', 'cada', 'placer', 'Los', 'divina', 'jamás', 'agua', 'patria', 'labios', 'hora', 'sí', 'eterno', 'rostro', 'seno', 'grande', 'aire', 'belleza', 'nuestra', 'mirada', 'después', 'has', 'santo', 'aliento', 'espíritu', 'puede', 'aurora', 'edad', 'fuente', 'noble', 'solo', 'destino', 'alegría', 'han', 'pobre', 'eterna', 'sé']\n",
      "[('de', 'la'), ('en', 'la'), ('en', 'el'), ('a', 'la'), ('y', 'el'), ('que', 'en'), ('de', 'su'), ('y', 'en'), ('de', 'mi'), ('que', 'el'), ('de', 'tu'), ('en', 'su'), ('y', 'de'), ('de', 'los'), ('de', 'un'), ('y', 'la'), ('de', 'las'), ('la', 'vida'), ('que', 'la'), ('en', 'tu'), ('a', 'su'), ('que', 'a'), ('el', 'alma'), ('en', 'mi'), ('lo', 'que'), ('y', 'al'), ('a', 'los'), ('que', 'no'), ('y', 'a'), ('en', 'las'), ('en', 'los'), ('a', 'tu'), ('el', 'cielo'), ('la', 'muerte'), ('de', 'una'), ('que', 'me'), ('el', 'sol'), ('que', 'al'), ('que', 'se'), ('de', 'amor'), ('el', 'mundo'), ('en', 'que'), ('a', 'mi'), ('por', 'el'), ('que', 'es'), ('la', 'tierra'), ('la', 'luz'), ('con', 'su'), ('de', 'tus'), ('con', 'la'), ('en', 'un'), ('la', 'noche'), ('que', 'te'), ('con', 'el'), ('que', 'de'), ('de', 'sus'), ('y', 'con'), ('por', 'la'), ('de', 'oro'), ('como', 'un'), ('a', 'las'), ('como', 'el'), ('y', 'que'), ('sobre', 'el'), ('en', 'sus'), ('y', 'no'), ('es', 'el'), ('el', 'mar'), ('y', 'su'), ('el', 'que'), ('a', 'quien'), ('más', 'que'), ('es', 'la'), ('de', 'mis'), ('el', 'corazón'), ('en', 'vano'), ('el', 'amor'), ('a', 'un'), ('al', 'fin'), ('en', 'tus'), ('con', 'que'), ('y', 'se'), ('no', 'es'), ('los', 'ojos'), ('mi', 'alma'), ('la', 'gloria'), ('el', 'viento'), ('en', 'ti'), ('y', 'tu'), ('mi', 'amor'), ('que', 'le'), ('mi', 'vida'), ('y', 'del'), ('el', 'dolor'), ('amor', 'y'), ('mi', 'corazón'), ('no', 'hay'), ('del', 'cielo'), ('tus', 'ojos'), ('la', 'que'), ('es', 'un'), ('ni', 'el'), ('un', 'día'), ('el', 'tiempo'), ('con', 'sus'), ('y', 'es'), ('que', 'con'), ('y', 'las'), ('del', 'mundo'), ('al', 'cielo'), ('y', 'los'), ('que', 'tu'), ('y', 'mi'), ('no', 'se'), ('luz', 'de'), ('con', 'tu'), ('y', 'por'), ('no', 'me'), ('Por', 'qué'), ('de', 'luz'), ('de', 'Dios'), ('que', 'por'), ('en', 'mis'), ('que', 'su'), ('la', 'sombra'), ('y', 'sin'), ('la', 'voz'), ('y', 'un'), ('a', 'ti'), ('bajo', 'el'), ('hasta', 'el'), ('a', 'sus'), ('ha', 'de'), ('como', 'una'), ('sobre', 'la'), ('el', 'día'), ('a', 'tus'), ('entre', 'las'), ('la', 'tarde'), ('que', 'si'), ('que', 'un'), ('de', 'que'), ('del', 'amor'), ('la', 'mano'), ('el', 'aire'), ('y', 'si'), ('del', 'alma'), ('el', 'bien'), ('las', 'flores'), ('la', 'frente'), ('que', 'mi'), ('cuando', 'el'), ('al', 'que'), ('a', 'Dios'), ('la', 'fe'), ('no', 'te'), ('al', 'mundo'), ('el', 'hombre'), ('y', 'me'), ('el', 'pecho'), ('en', 'lo'), ('ya', 'no'), ('el', 'suelo'), ('Y', 'en'), ('en', 'mí'), ('la', 'esperanza'), ('la', 'luna'), ('entre', 'el'), ('de', 'ser'), ('que', 'yo'), ('si', 'el'), ('tu', 'amor'), ('del', 'sol'), ('mi', 'pecho'), ('que', 'del'), ('los', 'que'), ('amor', 'que'), ('para', 'el'), ('de', 'aquel'), ('En', 'la'), ('vida', 'y'), ('de', 'lo'), ('por', 'los'), ('si', 'en'), ('bien', 'que'), ('la', 'memoria'), ('a', 'mis'), ('todo', 'el'), ('es', 'de'), ('como', 'la'), ('de', 'este'), ('en', 'tanto'), ('De', 'la'), ('el', 'fuego'), ('de', 'mí'), ('con', 'un'), ('y', 'te'), ('todos', 'los'), ('la', 'suerte'), ('de', 'ti'), ('En', 'el'), ('con', 'las'), ('que', 'los'), ('que', 'ha'), ('en', 'una'), ('y', 'más'), ('la', 'paz'), ('el', 'agua'), ('el', 'llanto'), ('a', 'lo'), ('y', 'sus'), ('por', 'qué'), ('en', 'ella'), ('la', 'aurora'), ('flor', 'de'), ('y', 'lo'), ('su', 'luz'), ('a', 'mí'), ('tu', 'frente'), ('para', 'que'), ('en', 'él'), ('tal', 'vez'), ('la', 'fama'), ('mis', 'ojos'), ('se', 'ha'), ('de', 'gloria'), ('de', 'esta'), ('la', 'virtud'), ('y', 'yo'), ('otra', 'vez'), ('tu', 'nombre'), ('por', 'las'), ('que', 'ya'), ('tu', 'alma'), ('la', 'patria'), ('por', 'ti'), ('si', 'no'), ('del', 'tiempo'), ('todo', 'lo'), ('con', 'los'), ('por', 'su'), ('sol', 'de'), ('son', 'de'), ('a', 'una'), ('tu', 'vida'), ('en', 'medio'), ('es', 'tu'), ('ante', 'el'), ('luz', 'que'), ('la', 'edad')]\n"
     ]
    }
   ],
   "source": [
    "# unigram and bigrams\n",
    "unigrams, bigrams = get_top_n_vocab(df_train['content'], n=250)\n",
    "print(unigrams)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a184f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the scansion is apparently horridly slow, so parallelize?\n",
    "# HONESTLY, still pretty slow so just don't use this code and work only with n-grams\n",
    "# out = df_train['content'].head(5).apply(get_metrical_vector)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4aac4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing feature vector 0/4045\n",
      "Constructing feature vector 1000/4045\n",
      "Constructing feature vector 2000/4045\n",
      "Constructing feature vector 3000/4045\n",
      "Constructing feature vector 4000/4045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mg/3jf2y33s1gl75f_3gb1l8sbh0000gn/T/ipykernel_44713/3158854010.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  input_arr = torch.Tensor(input).reshape((total, -1))\n"
     ]
    }
   ],
   "source": [
    "# compute ngram and prosodic features\n",
    "# N x D\n",
    "input = []\n",
    "counter = 0\n",
    "total = df_train.count()['content']\n",
    "for text in df_train['content']:\n",
    "    bow = text_to_bag_of_words(text, unigrams, bigrams)\n",
    "    input.append(bow)\n",
    "    #prosodic = get_metrical_vector(text)\n",
    "    #input.append(list(bow) + list(prosodic))\n",
    "    if counter % 1000 == 0:\n",
    "        print(f\"Constructing feature vector {counter}/{total}\")\n",
    "    counter += 1\n",
    "\n",
    "input_arr = torch.Tensor(input).reshape((total, -1))\n",
    "df_train['gender'] = df_train['gender'].map({'M': 0, 'F': 1})\n",
    "\n",
    "\n",
    "label_arr = torch.Tensor(df_train['gender'].values).reshape((-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c138f86",
   "metadata": {},
   "source": [
    "Create SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a48c13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0240, -0.0835],\n",
      "        [-0.0240, -0.0835],\n",
      "        [-0.0240, -0.0835],\n",
      "        ...,\n",
      "        [-0.0240, -0.0835],\n",
      "        [-0.2448,  0.0877],\n",
      "        [-0.0240, -0.0835]], grad_fn=<IndexBackward0>)\n",
      "torch.Size([4045, 2])\n",
      "torch.Size([4045, 2])\n",
      "torch.Size([4045, 2])\n",
      "torch.Size([4045, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (4045) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m margins = torch.max(diff + \u001b[32m1.0\u001b[39m, \u001b[32m0\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#### WHAT'S HAPPENING\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m loss = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhinge_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_arr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Hinge Loss is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     25\u001b[39m optimizer = torch.optim.SGD(model.parameters(), lr=\u001b[32m0.01\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/profiling-spanish-poets/models/svm_classifier.py:24\u001b[39m, in \u001b[36mMulticlassSVM.hinge_loss\u001b[39m\u001b[34m(self, outputs, labels, margin)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(labels.shape)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Compute the margin-based loss for all classes except the correct class\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m margins = torch.max(\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect_class_scores\u001b[49m + margin, \u001b[32m0\u001b[39m)\n\u001b[32m     25\u001b[39m margins[\u001b[38;5;28mrange\u001b[39m(num_samples), labels] = \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# Ignore the correct class\u001b[39;00m\n\u001b[32m     26\u001b[39m loss = torch.sum(margins) / num_samples\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (2) must match the size of tensor b (4045) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MulticlassSVM(input_arr.shape[1], 2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-3)\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(input_arr)\n",
    "\n",
    "    correct_class_scores = outputs[range(2), label_arr.int()]\n",
    "\n",
    "    print(correct_class_scores)\n",
    "        \n",
    "    # Compute the margin-based loss for all classes except the correct class\n",
    "    print(outputs.shape)\n",
    "    print(correct_class_scores.shape)\n",
    "    diff = outputs - correct_class_scores\n",
    "    margins = torch.max(diff + 1.0, 0, keepdim=True)\n",
    "\n",
    "\n",
    "    #### WHAT'S HAPPENING\n",
    "    loss = model.hinge_loss(outputs, label_arr.int())\n",
    "    print(f'Epoch {epoch}: Hinge Loss is {loss.item()}')\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87f25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9a4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
